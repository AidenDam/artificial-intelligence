{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_drama_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1FxBlZH1hZ23RPzLxdZF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsorTheDoctor/Sekcja-SI/blob/master/neural_networks/RNN/drama_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S37DURHPPpAk",
        "colab_type": "text"
      },
      "source": [
        "# RNN Drama Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPseahr9N3Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmxmpHXQMWC",
        "colab_type": "text"
      },
      "source": [
        "### Zbiór danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koNJD1FEQJm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNAC32sORDX_",
        "colab_type": "text"
      },
      "source": [
        "### Załadowanie własnych danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXT4AW4qRKbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmBl4bQORvv_",
        "colab_type": "text"
      },
      "source": [
        "### Odczytanie treści pliku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRhrtqdPR00-",
        "colab_type": "code",
        "outputId": "e56807e1-7352-4ceb-bcf7-18e7edf9f3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Przeczytanie i zakodowanie do formatu py2\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print('Długość tekstu: {} znaków'.format(len(text)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Długość tekstu: 1115394 znaków\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIZHL61hSeql",
        "colab_type": "code",
        "outputId": "88ba7594-5517-4515-f1fd-e5951618128c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "# Rzućmy okiem na pierwsze 250 znaków w tekście\n",
        "print(text[:250])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IYo_rB6SzV9",
        "colab_type": "text"
      },
      "source": [
        "### Kodowanie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOrIPWtXS2Wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Stworzenie mapowania unikatowych znaków do wskaźników\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvIgwCbmUHVx",
        "colab_type": "code",
        "outputId": "b530a6bb-6d82-46f7-d312-8c440b29bced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# zobaczmy na przykładzie jak nasz tekst został zakodowany\n",
        "print('Text:', text[:13])\n",
        "print('Encoded', text_to_int(text[:13]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7sNZfI0UoUY",
        "colab_type": "code",
        "outputId": "a347dc73-7710-40c2-f323-abc3aed0dccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgJOt6xCWFS1",
        "colab_type": "text"
      },
      "source": [
        "### Tworzenie przykładów treningowych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNJV7ZOXWFBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Tworzenie przykładów treningowych\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hExawNCaV_Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N733gEf5XaxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):  # na przykład: hello\n",
        "  input_text = chunk[:-1]  # hell\n",
        "  target_text = chunk[1:]  # ello\n",
        "  return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # używamy mapowania do zastosowania powyższej funkcji do każdego wpisu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5t8EEdwaVhv",
        "colab_type": "code",
        "outputId": "74f32724-e27f-42c3-e63f-699d3f1d98d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print('\\n\\nEXAMPLE\\n')\n",
        "  print('INPUT')\n",
        "  print(int_to_text(x))\n",
        "  print('\\nOUTPUT')\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zNVF2KtbVLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab to liczba unikatowych znaków\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg850Ag_cNqQ",
        "colab_type": "text"
      },
      "source": [
        "### Budowa modelu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GyVEYTScPPw",
        "colab_type": "code",
        "outputId": "7d162303-24d7-433f-b126-eafc1f7658e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSjyCZEeWJp",
        "colab_type": "text"
      },
      "source": [
        "### Tworzenie funkcji straty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jpBeoCGeZiz",
        "colab_type": "code",
        "outputId": "370a5aac-43cd-45b1-b81b-e51d56e12d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl67ROd9fguO",
        "colab_type": "code",
        "outputId": "0124da60-d23f-486d-bd60-0da4d0f73c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# możemy zobaczyć, że predykcja jest tablicą 64 tablic, po jednym wpisie na wsad\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.2756020e-03  1.0941733e-03  9.9033809e-05 ... -1.8608464e-03\n",
            "    1.0167472e-03 -5.2739895e-04]\n",
            "  [-4.9946303e-03  4.4669895e-03 -3.8288368e-03 ... -4.7846194e-03\n",
            "    2.7665924e-03 -2.8478878e-03]\n",
            "  [-2.4771232e-03  3.1500990e-03 -1.3275494e-02 ... -3.0980220e-03\n",
            "    2.9893944e-03 -3.8632054e-03]\n",
            "  ...\n",
            "  [-5.5281585e-03 -1.1321435e-02  4.3290039e-03 ...  8.0144862e-03\n",
            "   -9.8271901e-04  9.7659379e-03]\n",
            "  [-1.8137688e-03 -1.5505778e-03  2.6749412e-03 ...  2.5844565e-03\n",
            "   -4.7952235e-03  1.0138634e-02]\n",
            "  [-4.4095283e-03 -2.2342233e-03  6.0934145e-03 ...  5.6611970e-03\n",
            "   -2.4895081e-03  9.8194769e-03]]\n",
            "\n",
            " [[-9.5171115e-04 -2.2133302e-03  2.8357576e-03 ...  3.3565369e-03\n",
            "    3.4633213e-05  1.3851903e-03]\n",
            "  [-3.5982074e-03  3.2581401e-03  6.0820347e-04 ... -2.9555429e-04\n",
            "    1.0745848e-04 -8.9187827e-04]\n",
            "  [-4.5260820e-03  8.5509857e-03 -7.4342676e-03 ...  2.6712918e-03\n",
            "   -2.2839077e-03 -1.4654216e-03]\n",
            "  ...\n",
            "  [-1.4074701e-04  3.2960174e-03 -4.4243103e-03 ...  7.0089829e-04\n",
            "   -4.2586681e-03  1.0227366e-02]\n",
            "  [-1.9390995e-03  9.7523647e-04 -6.5834238e-03 ... -3.8506198e-03\n",
            "   -1.6134729e-04  8.5574165e-03]\n",
            "  [-7.6698223e-03 -3.3902228e-04  3.0704413e-04 ...  2.6566882e-05\n",
            "   -1.7491679e-03  6.8955333e-03]]\n",
            "\n",
            " [[ 3.3452297e-03 -5.7088463e-03  4.6377829e-03 ...  2.9262986e-03\n",
            "   -1.2239739e-03  3.1915167e-03]\n",
            "  [-4.0638560e-04 -4.2429771e-03  2.4218773e-03 ...  5.0668102e-03\n",
            "   -4.8432369e-03  7.0162793e-03]\n",
            "  [-1.2701241e-03 -5.4922095e-03  5.3252317e-03 ...  7.1544857e-03\n",
            "   -3.6173051e-03  6.2977658e-03]\n",
            "  ...\n",
            "  [-3.6046291e-03  3.6630533e-03 -2.7150696e-03 ...  4.0274845e-03\n",
            "   -4.5656008e-03  1.1819614e-02]\n",
            "  [-6.4238445e-03  6.2345797e-03 -6.5741614e-03 ... -2.2470455e-03\n",
            "    1.2869107e-03  8.5099004e-03]\n",
            "  [-8.1639644e-03  8.2708737e-03 -9.1844862e-03 ... -6.9813966e-03\n",
            "    5.4878290e-03  6.6353818e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.2240681e-03 -3.6307273e-03  7.7322032e-03 ...  1.3757236e-03\n",
            "    2.0402099e-03 -2.2420618e-03]\n",
            "  [-1.6547315e-03  2.5953553e-03  8.2304431e-03 ... -1.8660686e-03\n",
            "    3.3940954e-04  5.1436503e-04]\n",
            "  [-4.7652749e-03  1.9252949e-03  8.5196514e-03 ... -3.6889364e-03\n",
            "   -2.8932856e-03  3.0373156e-03]\n",
            "  ...\n",
            "  [-8.7348446e-03 -3.5283987e-03 -4.2740093e-03 ...  1.3783495e-03\n",
            "   -2.7523239e-03  6.7332885e-03]\n",
            "  [-8.0350852e-03 -6.4642099e-03  4.1056387e-03 ...  3.2003762e-03\n",
            "   -4.4381446e-03  7.5953454e-03]\n",
            "  [-6.5053212e-03 -9.6305842e-03 -2.9872992e-04 ...  1.4837144e-03\n",
            "   -8.3794706e-03  8.1571657e-03]]\n",
            "\n",
            " [[-9.5171115e-04 -2.2133302e-03  2.8357576e-03 ...  3.3565369e-03\n",
            "    3.4633213e-05  1.3851903e-03]\n",
            "  [-2.8684933e-03 -2.8050765e-03 -1.8634675e-03 ...  4.1112900e-03\n",
            "   -8.8527454e-03  2.7356960e-03]\n",
            "  [-4.1055153e-04 -7.8986585e-04 -2.3528442e-03 ...  3.7569311e-03\n",
            "   -4.9375161e-03  1.7144496e-03]\n",
            "  ...\n",
            "  [-1.3233585e-03  3.5535055e-03  5.3002308e-03 ...  4.0608407e-03\n",
            "   -2.3684513e-03  9.1074267e-03]\n",
            "  [-3.8420719e-03  1.0662777e-03  7.9442272e-03 ...  7.7577285e-03\n",
            "   -1.5846528e-04  8.9509580e-03]\n",
            "  [-3.4216403e-03 -4.5419075e-03  7.8817196e-03 ...  1.3883704e-05\n",
            "    2.3985044e-03  1.7879907e-02]]\n",
            "\n",
            " [[-9.5171115e-04 -2.2133302e-03  2.8357576e-03 ...  3.3565369e-03\n",
            "    3.4633213e-05  1.3851903e-03]\n",
            "  [ 1.5417977e-03 -7.2626234e-03  5.9329192e-03 ...  3.7145526e-03\n",
            "   -1.8061488e-03  4.4222390e-03]\n",
            "  [-1.3898826e-03 -4.4011888e-03  4.8394231e-03 ... -2.4124795e-04\n",
            "    2.3380564e-04  3.7604789e-03]\n",
            "  ...\n",
            "  [-5.3111059e-03 -1.0637480e-02  6.5504103e-03 ... -3.5906311e-03\n",
            "   -4.9741156e-03  1.4668833e-02]\n",
            "  [-7.6128938e-03 -6.9868523e-03  5.0835358e-03 ... -6.1079301e-03\n",
            "   -1.7938318e-03  1.1807417e-02]\n",
            "  [-7.3849256e-03 -8.3774310e-03  7.6148752e-03 ... -4.6120021e-03\n",
            "   -1.1899428e-04  1.6007125e-02]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6RTGfO0gigI",
        "colab_type": "code",
        "outputId": "7dc612e4-a03b-4029-a747-a8da8b50c6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "# sprawdźmy pojedynczą predykcję\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-1.2756020e-03  1.0941733e-03  9.9033809e-05 ... -1.8608464e-03\n",
            "   1.0167472e-03 -5.2739895e-04]\n",
            " [-4.9946303e-03  4.4669895e-03 -3.8288368e-03 ... -4.7846194e-03\n",
            "   2.7665924e-03 -2.8478878e-03]\n",
            " [-2.4771232e-03  3.1500990e-03 -1.3275494e-02 ... -3.0980220e-03\n",
            "   2.9893944e-03 -3.8632054e-03]\n",
            " ...\n",
            " [-5.5281585e-03 -1.1321435e-02  4.3290039e-03 ...  8.0144862e-03\n",
            "  -9.8271901e-04  9.7659379e-03]\n",
            " [-1.8137688e-03 -1.5505778e-03  2.6749412e-03 ...  2.5844565e-03\n",
            "  -4.7952235e-03  1.0138634e-02]\n",
            " [-4.4095283e-03 -2.2342233e-03  6.0934145e-03 ...  5.6611970e-03\n",
            "  -2.4895081e-03  9.8194769e-03]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2zWNZGwuZxh",
        "colab_type": "code",
        "outputId": "affa9a8c-e7a2-4fd7-821d-644241bce292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "# spójrzmy na predykcję z pierwszego kroku\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# jest tu 65 wartości reprezentowanych przez prawdopodobieństwo wystąpienia każdego znaku jako następny"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.2756020e-03  1.0941733e-03  9.9033809e-05 -3.7100830e-04\n",
            "  5.6809927e-03 -7.4793526e-04 -9.2137590e-05 -2.6758232e-03\n",
            "  1.5958657e-03 -1.8048374e-03 -3.5606974e-03  1.8626013e-03\n",
            " -4.3759678e-04  2.7684614e-03 -2.6411107e-03 -2.3570445e-03\n",
            "  2.8696519e-03 -5.5329211e-04 -3.0692404e-03  3.2588774e-03\n",
            " -2.6513748e-03  1.1792282e-03  3.9605140e-03  9.3658920e-03\n",
            "  1.7634440e-05  4.3875352e-03  4.2639792e-04  2.0342353e-03\n",
            " -3.1153311e-04 -2.5496816e-03 -1.0030870e-03  2.3485881e-03\n",
            "  1.4894512e-03 -2.5002579e-03  3.4288352e-04  1.2853282e-03\n",
            "  1.7979086e-03 -3.2415984e-03 -4.9822492e-04 -1.0975995e-03\n",
            " -4.8717051e-03 -3.2724312e-03  4.2283107e-03 -1.7875977e-03\n",
            "  3.7031602e-03 -6.1697913e-03 -4.7452246e-05 -2.2232460e-03\n",
            " -9.6392490e-05 -9.8107522e-04 -3.1455180e-03 -4.2782456e-04\n",
            "  2.3093324e-03 -1.1087297e-03  7.4675274e-03  2.2986417e-03\n",
            " -7.7618472e-04  1.4552096e-03  4.5798169e-03  4.5486977e-03\n",
            "  6.1028539e-03 -1.5859671e-04 -1.8608464e-03  1.0167472e-03\n",
            " -5.2739895e-04], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zheuw-skhkw6",
        "colab_type": "code",
        "outputId": "57503d3f-561a-4150-b427-5e5db1ba2acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"DRieLPnhj!JixCDKJ kDPUlxf?E-yPUhWDrOMbkgfY&VeFsa:, g'Ii&X$3CYsHCcMouTapnrDq;zJ:cW;yrIeJd!3yh&FSUcvc'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gPXpkqQidVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hen4V8mi9en",
        "colab_type": "text"
      },
      "source": [
        "### Kompilacja modelu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdYESE2yjAOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1q4EM0njMSm",
        "colab_type": "text"
      },
      "source": [
        "### Tworzenie checkpointów"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvBDJ-pAjPln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Słownik, w którym checkpointy będą zapisywane\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Nadanie nazwy dla plików checkpoint\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTySf33kHWL",
        "colab_type": "text"
      },
      "source": [
        "### Trening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy0NvA7jcGA",
        "colab_type": "code",
        "outputId": "5742c339-1336-4bf4-a358-bd17c7bbaf64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 34s 198ms/step - loss: 2.5871\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.8930\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.6438\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.5093\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 32s 189ms/step - loss: 1.4263\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.3688\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.3238\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 1.2850\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.2480\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 1.2123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRjsNomZk5y3",
        "colab_type": "text"
      },
      "source": [
        "### Załadowanie modelu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Ck19cQk_CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI_rSfNOlRyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ijZBN8ls32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint_num = 10\n",
        "# model.load_weights(tf.train.load_checkpoint('./training_checkpoints/ckpt_' + str(checkpoint_num)))\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykotQ3XHmLUg",
        "colab_type": "text"
      },
      "source": [
        "### Generowanie tekstu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F4MXOllmPfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  # liczba znaków do wygenerowania\n",
        "  num_generate = 800\n",
        "\n",
        "  # konwersja stringów na liczby (wektoryzacja)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # pusty string do przechowania naszych wyników\n",
        "  text_generated = []\n",
        "\n",
        "  # Niższe temperatury powodują bardziej przewidywalny tekst.\n",
        "  # Wyższe temperatury powodują bardziej niespodziewany tekst.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # usunięcie wymiaru wsadu\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # użycie rozkładu kategorycznego do predykcji znaku zwróconego przez model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # Przepuszczamy przewidziany znak jako następne wejście do modelu\n",
        "    # wraz z poprzednim ukrytym stanem\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCb3W0H4sQaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "b31c4ea4-9229-4ea1-c963-27673be543b7"
      },
      "source": [
        "inp = input('Wpisz łańcuch początkowy: ')\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wpisz łańcuch początkowy: be or not to be\n",
            "be or not to bed your lord.\n",
            "\n",
            "LUCENTIO:\n",
            "Farewell: I hopper else I am\n",
            "I will revenge you to our life unto his urboor fortunce, and a curst harrial men.\n",
            "\n",
            "ROMEO:\n",
            "Not with me o penine, till what do for the water,\n",
            "As him and whine woman! I will find me to\n",
            "me not, sir.\n",
            "\n",
            "MERCUTIO:\n",
            "Hence,!\n",
            "O meeping means are alrosled, I thank you your arms!\n",
            "Soul should he said that is the river and resign ones,\n",
            "No charges with debt. But how did you Richard stand ourselves, as the words of mine:'\n",
            "For now that I might find thee to a word;\n",
            "So, now for mad I give it not to hear.\n",
            "\n",
            "KING RICHARD II:\n",
            "Nay, I will die in these;\n",
            "For that we shall not, hirse! some deally\n",
            "place I mean, that they safell'd I will cheer;\n",
            "And so pray you with marrial death.\n",
            "\n",
            "Third Citizen:\n",
            "To-morrow looks; which I think you hence to he\n",
            "All then gone time:\n",
            "Since \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ojlCkYsoHz",
        "colab_type": "text"
      },
      "source": [
        "### Źródło:\n",
        "[https://www.youtube.com/watch?v=tPYj3fFJGjk](https://www.youtube.com/watch?v=tPYj3fFJGjk)"
      ]
    }
  ]
}